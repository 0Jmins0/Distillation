nohup: ignoring input
/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Using device: cuda
loading train data.......
finished loading train data.......
finished loading train data.......
Training MV_AlexNet

init ALEXNET model...

No saved model found. Starting training from scratch.
before train
Epoch 1/40:   0%|          | 0/34 [00:00<?, ?batch/s]Epoch 1/40:   0%|          | 0/34 [00:02<?, ?batch/s]
Traceback (most recent call last):
  File "/home/xyzhang/project/Distillation/src/train.py", line 197, in <module>
    positive_features = model(positive_images)
  File "/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/xyzhang/project/Distillation/src/models/Students/MVAlexNet.py", line 156, in forward
    x = self.features(x)
  File "/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/xyzhang/project/Distillation/src/models/Students/MVAlexNet.py", line 36, in forward
    features = self.features(x) # 13 * 13 * 256
  File "/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torch/nn/modules/pooling.py", line 213, in forward
    return F.max_pool2d(
  File "/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torch/_jit_internal.py", line 624, in fn
    return if_false(*args, **kwargs)
  File "/home/xyzhang/anaconda3/envs/Distillation/lib/python3.9/site-packages/torch/nn/functional.py", line 830, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 76.19 MiB is free. Process 499389 has 14.33 GiB memory in use. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Process 500624 has 1.55 GiB memory in use. Process 500623 has 2.43 GiB memory in use. Process 500625 has 2.05 GiB memory in use. Of the allocated memory 2.88 GiB is allocated by PyTorch, and 50.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
